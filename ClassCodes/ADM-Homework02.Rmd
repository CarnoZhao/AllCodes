---
title: "Applied Data Mining Homework 2"
author: "Xun Zhao, xz2827"
output: pdf_document
fontsize: 12pt
---

# Problem 1: Trees

## 1.

Yes. But it needs the tree to be very high than some simple tree classifiers.

As the tree classifier boundary is made of some line segments that are parallel with x-axis or y-axis, we can draw a zigzag curve along the linear boundary. Thus, the sloping linear boundary becomes many segments, and every segment is a dicision in tree classifier.

```{r, echo = FALSE, out.width = "20em", fig.align = "center"}
x1 = rnorm(100, 0, 1)
x2 = rnorm(100, 0, 1)
y = ifelse(x1 + x2 > 0, 1, 0)
d = data.frame(cbind(x1, x2, y)[which(x1 + x2 > 1 | x1 + x2 < -1),])
library(ggplot2)
ggplot(d, aes(x = x1, y = x2, color = as.factor(y))) + 
	geom_point() + 
	geom_segment(x = -2, y = 2, xend = -1, yend = 2) +
	geom_segment(x = -1, y = 2, xend = -1, yend = 1) +
	geom_segment(x = -1, y = 1, xend = 0, yend = 1) +
	geom_segment(x = 0, y = 1, xend = 0, yend = 0) +
	geom_segment(x = 0, y = 0, xend = 1, yend = 0) +
	geom_segment(x = 1, y = 0, xend = 1, yend = -1) +
	geom_segment(x = 1, y = -1, xend = 2, yend = -1) +
	geom_segment(x = 2, y = -1, xend = 2, yend = -2)
```

## 2.

No, the Bayes-Optimal is defined as:

$$f(\vec{x}) = \underset{y}{arg\,max}P({\bf{Y}} = y | {\bf{X}} = \vec{x})$$

And the risk is defined as:

$$R(f) = \underset{+1,\,-1}{\sum}\int L(y,\,f(\vec{x}))P(\vec{x},\,y)d\vec{x}$$

Thus, the risk $R(f)$ depends on the possible data distribution instead of only existing data points. Moreover, the Bayes-optimal classifier is defined by the possible distribution, so it can always minimize the risk under certain data model. However, the tree classifier cannot fit the linear boundary, which causes the misclassification compared with Bayes-optimal. As the Bayes-optimal has the lowest risk, the risk of tree classifier will always differ from Bayes-optimal, unless the Bayes-optimal's boundary is axis-parallel.

## 3. 

$$g(\vec{x}) = \begin{cases}
	f_1(\vec{x}) > 0 &
	\begin{cases}
		f_2(\vec{x}) > 0 & class\,A\\\
		f_2(\vec{x}) < 0 & class\,B
	\end{cases}\\\
	f_1(\vec{x}) < 0 &
	\begin{cases}
		f_3(\vec{x}) > 0 & class\,B\\\
		f_3(\vec{x}) < 0 & class\,C
	\end{cases}
\end{cases}$$

# Problem 2: 10-fold Cross Validation

## 1.

10 folds means that we devide the data set $\chi$ in to 10 parts with the same size $\frac{||\chi||}{10}$.
$$\chi = \bigcup^{10}_{i = 1}\chi_i$$
$$\text{if }i\neq j,\,\chi_i\cap\chi_j = \varnothing$$
$$||\chi_i|| = \frac{||\chi||}{10}$$

## 2.

Because k-NN algorithm actually is not a learning algorithm with model parameter, the training data can be used only for classification. Thus, we do not have to choose the best model from a set.

Every time we use a new fold, namely, different cross validation set $\chi_{cv}$, we apply the classification function $f$ on $\chi_{cv}$ based on training set $\chi_{tr} = \chi-\chi_{cv}$ and culculated the error.

The training set is used as model (or classification function).

$$\hat{y} = f(\vec{x};\,k,\,\chi_{training}),\ \ \ k,\,\chi_{training}\text{ are fixed}$$

The cross validation set is used as a criterion to quantify how well the model can generalize.

$$Risk = \sum_{\vec{x}\text{ in }\chi_{cv}}l[y,\,f(\vec{x};\,k,\,\chi_{training})]$$

## 3.

As the risk function $R(k)$ given below, we compare the risk (for all folds and for all cross validation data points).

$$R(k) = \frac{1}{10}\sum^{10}_{i = 1}\frac{1}{||\chi_i||}\sum^{||\chi_i||}_{j = 1}l[y_{ij},\,f(\vec{x}_{ij};\,k,\,\chi - \chi_i)]$$

## 4.

We can simply choose $k$ that minimize the risk $R(k)$.

$$k = \underset{k\in \lbrace 1, 3, 5, 7, 9\rbrace}{arg\,min}R(k)$$

## 5.

The most obvious disadvantage of k-NN algorithm is that it has to store all the training data, and iterate all the training data when classifying a new data point. Thus, it is time-consuming and occupy a lot of storage.